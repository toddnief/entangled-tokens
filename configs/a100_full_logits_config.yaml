# A100 Configuration for Full Vocabulary Logits Extraction
# Optimized for 80GB A100 GPU with file splitting for manageable sizes

# Model settings
model_name: "meta-llama/Llama-2-7b-hf"  # Or Llama-2-13b-hf for larger model
backend: "transformers"  # Use transformers for full logits access
dtype: "float16"  # Use FP16 for memory efficiency
max_memory_gb: 70  # Leave 10GB free on 80GB A100

# Dataset settings
dataset_name: "Bingsu/openwebtext_20p"
num_samples: 1000
chunk_size: 512  # Maximum sequence length (tokens)
split: "train"

# Logits extraction settings  
top_k: 5000  # Top-k alternatives per position (instead of full 32k vocab)
return_full_vocab: false  # Set to true for complete vocabulary (very large files)

# File management settings
output_dir: "full_logits_outputs"  # Directory to save results
samples_per_file: 50  # Split files every N samples
max_file_size_mb: 500  # Split files if they exceed this size
compress: true  # Enable compression for storage efficiency

# Performance settings
batch_size: 1  # Process one sample at a time for memory efficiency
gradient_checkpointing: true  # Reduce memory usage
use_cache: false  # Don't cache intermediate results
memory_cleanup_interval: 50  # Clear GPU cache every N samples

# Advanced A100 optimizations
torch_compile: false  # Enable for PyTorch 2.0+ (experimental)
flash_attention: false  # Enable if available
tensor_parallel: false  # For multi-GPU setup

# Safety limits
max_sequence_length: 2048  # Hard limit to prevent OOM

# Logging
verbose: true
log_level: "INFO"
progress_bar: true

# File naming and organization
save_config: true  # Save config with each run
create_run_dirs: true  # Create timestamped directories for each run
save_summary: true  # Create summary files for each run